import torch
import torch.utils.data
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
from generate_dataset import get_dataset


def draw_confusion_matrix(confusion_mat, labels):
    """
    This function draws a confusion matrix for binary classification.
    """
    plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.colorbar()
    tick_marks = np.arange(len(labels))
    plt.xticks(tick_marks, labels, rotation=45)
    plt.yticks(tick_marks, labels)

    # Print the confusion matrix values on the chart
    for i in range(len(labels)):
        for j in range(len(labels)):
            plt.text(j, i, format(confusion_mat[i, j], 'd'),
                     horizontalalignment="center",
                     color="white" if confusion_mat[i, j] > confusion_mat.max() / 2. else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()
    plt.show()


def test_model(model_path, test_benign_path, test_malignant_path):
    """
    This function tests the model and prints out the performance metrics.
    """
    dataset = get_dataset(test_benign_path, test_malignant_path)
    test_loader = torch.utils.data.dataloader.DataLoader(dataset, batch_size=28)

    # Load model
    model = torch.load(model_path).cuda()
    model.eval()

    y_true = []
    y_pred = []

    # No gradient needed for evaluation
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs = inputs.cuda()
            outputs = model(inputs)
            predicted_probs = torch.sigmoid(outputs)
            predicted_labels = (predicted_probs > 0.5).float()
            y_pred.extend(predicted_labels.view(-1).cpu().numpy())
            y_true.extend(labels.cpu().numpy())

    # Calculate metrics
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    # Generate confusion matrix
    conf_mat = confusion_matrix(y_true, y_pred)
    draw_confusion_matrix(conf_mat, labels=['Benign', 'Malware'])

    # Print performance metrics
    print('Confusion Matrix:\n', conf_mat)
    print('Accuracy: {:.4f}'.format(accuracy))
    print('Precision: {:.4f}'.format(precision))
    print('Recall: {:.4f}'.format(recall))
    print('F1 Score: {:.4f}'.format(f1))

# Example usage
# You should replace 'path_to_saved_model.pth', 'path_to_test_benign_data', 'path_to_test_malignant_data' with your actual paths
# test_model('path_to_saved_model.pth', 'path_to_test_benign_data', 'path_to_test_malignant_data')
